---
layout: post
title: "Judgement vs Decision Making "
author: [potatowire]()
categories: 
tags: 
banner: 
caption: 
published: false
---

> The distinction between judgment and decision making appeared as fuzzy as the distinction between judgment and prediction. But to [Amos][2], as to other mathematical psychologists, they were distinct fields of inquiry. A person making a judgment was assigning odds. How likely is it that that guy will be a good NBA player? How risky is that triple-A-rated subprime mortgage–backed CDO[^1]? Is the shadow on the X-ray cancer? Not every judgment is followed by a decision, but every decision implies some judgment. The field of decision making explored what people did after they had formed some judgment—after they knew the odds, or thought they knew the odds, or perhaps had judged the odds unknowable. Do I pick that player? Do I buy that CDO? Surgery or chemotherapy? It sought to understand how people acted when faced with risky options.[^2]

> The Oregon researchers began by creating, as a starting point, a very simple algorithm, in which the likelihood that an ulcer was malignant depended on the seven factors the doctors had mentioned, equally weighted. The researchers then asked the doctors to judge the probability of cancer in ninety-six different individual stomach ulcers, on a seven-point scale from “definitely malignant” to “definitely benign.” Without telling the doctors what they were up to, they showed them each ulcer twice, mixing up the duplicates randomly in the pile so the doctors wouldn’t notice they were being asked to diagnose the exact same ulcer they had already diagnosed. The researchers didn’t have a computer. They transferred all of their data onto punch cards, which they mailed to UCLA, where the data was analyzed by the university’s big computer. The researchers’ goal was to see if they could create an algorithm that would mimic the decision making of doctors. [Kindle link](http://a.co/iV6KJg6)
> 
> This simple first attempt, Goldberg assumed, was just a starting point. The algorithm would need to become more complex; it would require more advanced mathematics. It would need to account for the subtleties of the doctors’ thinking about the cues. For instance, if an ulcer was particularly big, it might lead them to reconsider the meaning of the other six cues. [Kindle link](http://a.co/049wvLy)
> 
> He explained that the Oregon Research Institute had completed a study of doctors. They had found a gaggle of radiologists at the University of Oregon and asked them: How do you decide from a stomach X-ray if a person has cancer? The doctors said that there were seven major signs that they looked for: the size of the ulcer, the shape of its borders, the width of the crater it made, and so on. The “cues,” Goldberg called them, as Hoffman had before him. There were obviously many different plausible combinations of these seven cues, and the doctors had to grapple with how to make sense of them in each of their many combinations. The size of an ulcer might mean one thing if its contours were smooth, for instance, and another if its contours were rough. Goldberg pointed out that, indeed, experts tended to describe their thought processes as subtle and complicated and difficult to model. [Kindle link](http://a.co/3L6YHjd)

> Pap smears are very useful for catching cervical cancer, and since the 1990s automated imaging systems have been used to screen pap slides. Image-searching software scans slides for signs of abnormal cells far more rapidly than a human could, but it’s not all about the machine. The software identifies the images that human experts need to look at more closely, and there is evidence that these man–machine collaborations outperform the humans working alone, whether in terms of accuracy or speed. [Kindle link](http://a.co/baVgZUa)
> 
> Machines can double-check human diagnoses, catch the mistakes of very tired doctors, and keep up with and store new developments in the medical literature. And by the way, that literature doubles in size every few years, much more rapidly than any human can follow. For all of our scientific and medical progress, misdiagnoses remain common. [Kindle link](http://a.co/cDBbVjQ)
> 
> Of course, it can be irresponsible to rely completely on the computer’s pattern recognition skills, since the human eye will pick up image errors or flubbed data inputs in a way that the machine may not. But a man–machine team is less vulnerable to machine oversights; human supervision is often stipulated by the companies that market medical software. [Kindle link](http://a.co/1TOlPE2)
> 
> The major dating services use artificial intelligence to recommend romantic matches, and this makes a difference because those selections very often differ from what individuals would have chosen for themselves. For instance, eHarmony has gone from analyzing a few dozen variables per user to six hundred variables per user (a figure that may well be obsolete by the time you are reading this). Those variables include not just profile information but also how frequently users log in, whom they search for, and which profiles they actually decide to write to. One problem, of course, is that a lot of the data are based on self-reporting, and the answers of the respondents may be careless or outright lies based in the hope of being offered better and more exclusive matches. The algorithms also tend to select for similarities, and that may encourage initial meetings, but it is less clear it will produce happier marriages thirty years down the line. [Kindle link](http://a.co/2dvCD8a)
> 
> The current case for the software is not that it can turn love into a statistically measurable algorithm. We’d be foolish to ignore the importance of conversational rhythms and sniffs and smells and sexual chemistry and how long-term compatibility evolves over the years and decades. Nonetheless, the software is getting users to look twice at some prospects they might otherwise have passed over or never seen in the first place. Here is one romance from Match.com: [Kindle link](http://a.co/07DFz9Z)
> 
> Viewing the profile, Cambry, who is black, saw a pretty young white woman who lived nearby and seemed to share his interest in music. He sent her a short e-mail to say hello, and within a day received an e-mail from Karrah O’Daniel, an opera singer. Their first date was a flop, but they made it to a second date, and soon Cambry and O’Daniel were getting serious. It turns out they had attended the same music school, but never met there. They took to playing pieces by Franz Liszt together, recording videos that they would post on YouTube. Six months later, he proposed. The two are to wed on October 1 at a church in Minnesota. [Kindle link](http://a.co/abPiX6v)
> 
> As often happens in love, the woman Cambry fell for is not exactly the woman he thought he wanted. “I wasn’t expecting that the person I was going to marry would be a white woman from Inver Grove Heights, Minnesota,” he says. Nor did Cambry fit neatly into O’Daniel’s idea of who she would marry. According to her Match profile, she was looking for a man between the ages of 21 and 26 [Cambry was then 28]. Cambry, by O’Daniel’s own standards, was too old for her. In fact, Cambry and O’Daniel never really searched for one another at all. They were introduced by the algorithm. [Kindle link](http://a.co/dQvMQCo)







[^1]:	[Collateralized debt obligation][3]; most Lewis analogies are either to finance or to sports.

[^2]:	Lewis, Michael. [*The Undoing Project: A Friendship That Changed Our Minds*]()(https://www.amazon.com/dp/B01GI6S7EK/?tag=potatowire-20). New York: W.W. Norton & Company, 2017. [Kindle link]()

[2]:	https://en.m.wikipedia.org/wiki/Amos_Tversky
[3]:	https://en.wikipedia.org/wiki/Collateralized_debt_obligation
